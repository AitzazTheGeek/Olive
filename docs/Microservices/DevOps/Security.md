# Securing your production environment with AWS

### Introduction
This document describes the process of deploying the application from the *development* environment to *production*.

Each microservice is treated as a separate project and the instructions on this document will be the same for all of them.

### Development
Almost every project these days will use GIT (or some other version control) to keep track of, and manage code changes. This document will assume that you use `Bitbucket`, but you can use any other repository provider as well.

For each application, we will create a project in Bitbucket. For each microservice in the application, we will create a separate repository in the project.

Developers will have write access only to the repositories of the microservices they are assigned to. 

#### _Security Caution_
You must NEVER store any sensitive information in the source code. Secure info such as connection strings, credentials, api keys etc, are injected to the application during the `deployment process`, which will be explained later in this article. 

### Continious Delivery
`Continious Delivery` and `DevOps` have many well known operational benefits for the process of software development and management. But they are even more vital in a Microservice-oriented architecture. Why? Because with Microservices, you will typically have `10s of deployments per day`, after every small bug fix, or new feature. That makes it impossible to go about it manually.

`Continious deployment` enables us to rapidly deploy changes without the risk of `human errors`. Once designed and tested, you can trust the deployment process way more than any developer, even the most experienced ones.

Also, `scripting the deployment process` is a form of `documenting` it, that enables anybody to learn and do the process easily.

### Jenkins
There are many tools out there for deployment automation but we have chosen `Jenkins` because:

- It is one of the most popular tools, which means there is a good community support for it.
- It is open source, so we don't have to pay for anything.
- It has a rich library of plugins for different build and deployment tasks.

Our build process has a fairly complex structure and chosing Jenkins has been nothing but benefitial for us (hopefully it will remain that way). Please make sure you review the Jenkins document [here](https://github.com/Geeksltd/Olive/blob/master/docs/Microservices/DevOps/Jenkins.md).

### Our Jenkins structure
For our current Jenkins structure, we have two servers:

- **Windows VM**: to run Jenkins on, and use it as the Windows build slave.
- **Linux VM**: to build our docker images.

#### Isolasion of environments: Build vs Production
Our goal is to create the build environment in secure way that is only accessible from the company's network. It is important to give the Jenkins servers the permission they need to build and deploy the code, and nothing more.

In other words, we want to keep the production environment away from the build environment. To achive that we have created a `separate VPC (Virtual Private Cloud)` on AWS, which hosts the Jenkins servers.

#### Restricted access to Build environment
The two Jenkins servers share a `security group` that has been updated to give access to port 80 to requests initiated from our local company network (IP address). That way, people from outside our company network cannot access the Jenkins instance.

#### Secure deployment info
Jenkins jobs require certain secure information to build and deploy the code. Those are stored as `Jenkins' Credentials` through the Jenkins UI in the admin section. Such secure information include:

- Git repository credentials
- ECR repository credentials
- Kubernetes api credentials
- ...

> TODO: We can use IAM Roles to give access to the build server to push images to ECR to avoid having to store ECR credentials on Jenkins. Also, other secure info can be put on the AWS Secrets Manager and pulled by Jenkins when building to again avoid storing credentials on Jenkins.

### Container Repository
Container images generated by our build process should be stored in a repository that is accissble by our production environment to pull and run. Like other areas there are different options we had to use as our repository, such as Docker hub or Google container registry or AWS ECR. We chose AWS ECR becauase our build and production environment are hosted on AWS so communication between them and the container registry will be faster and we can manage access permissions using role as opposed to storing and using secure credentials. 
For each microservice we have created a separate container repository in the eu-west-1 region. 

### Runtime Secure info
As mentioned earlier we should avoid putting secure information in the code. One of the challenges we had was to find a way to securly pass secure information to the application in a way that cannot be hacked easily. 

#### AWS IAM
AWS offers the IAM service for managing access permissions to other AWS resources. Using IAM you can tailor a role for each service with minimum permissions required. Because IAM roles are defined and assigned in AWS level there is no need to deal with generating and storing credentials.

#### AWS IAM Role for Kubernetes
IAM roles can only be assigned to AWS resources, hence Kubernetes currently only supports assigning them to nodes.However, assigning a role to a node means that all processes running on that node will share the same role, which from the security point of view is not ideal asthe node may host different pods which require different permissions and we do not want have a super role in the system and share it between pods.  There are some solutions suggested by the community to assign a role to a pod but we didn’t want to add more elements to our cluster. We try to keep our cluster as simple as possible so we came up with a solution so that we can assign IAM roles directly to pods without having the overhead of using other services on Kubernetes. 

#### AWS IAM Role for Pods
As mentioned earlier, pods run on nodes and each node has a role on AWS, therefore the running pods share the same role as the hosting node. There is a feature in AWS IAM which enables a user or a resource with an IAM role to assume another role by creating a trust relationship between the assuming and assumed roles. Another feature that AWS IAM offers is to generate temporary credentials for each role. Using these two features we managed to assign the service IAM role we created to our K8s pod. For that we had to create a trust relationship between the node IAM role and the service IAM role. To assign the service IAM role to each pod we added a new environment variable to the service deployment template whose value is read from a K8s secret we created for the service. When a pod starts, the application reads the service IAM role from the environment variable, uses the AWS sdk to assume the service role, and then generates a temporary credentials for the service role. This way we can set the timespan of the temporary credentials to as short as 1 hr and either generate new credentials in a scheduled background process or even better, re-create new pods automatically. Bear in mind that it takes the application some time to generate new credentials and be ready to serve traffic, so you may want to implement kubernetes “pod readiness” to wait for the pod to be ready before directing traffic to it.
The benefit of generating temporary credentials is that you can store them in memory instead of environment variables which are not secure and can be viewed.

