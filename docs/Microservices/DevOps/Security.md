# Securing your production environment with AWS

### Introduction
This document describes the process of deploying the application from the development environment to deployment to production and runtime. Each microservice is treated as a separate project and the instructions on this document will be the same for all of them.


### Development
These days it is common sense to use some sort of version control for projects to be able to keep track of and manage code changes. We have chosen Bitbucket as our repository TODO: reason?. For each application we create a project in Bitbucket and for each microservice in the application we create a separate repository in the project. Developers will have write access only to the repositories of the microservices they are assigned to. 

#### _Security Caution_
You need to make sure you do not put any sensitive information in the source code. Secure info such as connection strings, credentials, api keys etc, are injected to the application during the deployment process, which will be explained later in this article. 

### Build
Microservice oriented architecture enables us to very reactive, meaning if there is a bug in a microservice, or if we need a new microservice we can take actions and update the application accordingly.However, this cannot be achived without automated deployment. Continious deployment enables us to rapidly deploy changes without the risk of human errors. Once designed and tested you can trust the deployment process way more than the most experienced developers. Also, scripting the deployment process is a form of documenting it that enables anybody to learn and do the process easily. There are many tools out there for deployment automation but we have chosen Jenkins because firstly, it is one of the most popular tools, which means there is a good community support for it, secondly, it is open source, so we don't have to pay for anything, and more importantly, has a rich library of plugins for different build and deployment tasks. Our build process has a fairly complex structure and chosing Jenkins has been nothing but benefitial for us (hopefully it will remain that way). Please make sure you review the Jenkins document [here](https://github.com/Geeksltd/Olive/blob/master/docs/Microservices/DevOps/Jenkins.md).
For our current Jenkins structure, we need to have two servers. One Windows instance to run Jenkins on and use it as the Windows build slave and a Linux instance to build our docker images. Our goal is to create the build environment in secure way that is only accessible from the company's network. It is important to give the Jenkins servers the permission the need to build and deploy the code and nothing more. In other words we want to keep the production environment away from the build environment. To achive that we have created a separate VPC on AWS which hosts the Jenkins servers. The Jenkins servers share a security group that has been updated to give access to port 80 to requests initiated from our local network address. That way people from outside our company network cannot access the Jenkins instance.
We have defined secure info the Jenkins jobs require to build and deploy the code (i.e. git repository credentials, ECR repository credentials, Kubernetes api credentials etc.) in Jenkins' Credentials.
TODO : We can use IAM Roles to give access to the build server to push images to ECR to avoid having to storing ECR credentials on Jenkins. Also, other secure info can be put on the AWS Secrets Manager and pulled by Jenkins when building to again avoid storing credentials on Jenkins.

### Container Repository
Container images generated by our build process should be stored in a repository that is accissble by our production environment to pull and run. Like other areas there are different options we had to use as our repository, such as Docker hub or Google container registry or AWS ECR. We chose AWS ECR becauase our build and production environment are hosted on AWS so communication between them and the container registry will be faster and we can manage access permissions using role as opposed to storing and using secure credentials. 
For each microservice we have created a separate container repository in the eu-west-1 region. 

### Runtime Secure info
As mentioned earlier we should avoid putting secure information in the code. One of the challenges we had was to find a way to securly pass secure information to the application in a way that cannot be hacked easily. 

#### AWS IAM
AWS offers the IAM service for managing access permissions to other AWS resources. Using IAM you can tailor a role for each service with minimum permissions required. Because IAM roles are defined and assigned in AWS level there is no need to deal with generating and storing credentials.

#### AWS IAM Role for Kubernetes
IAM roles can only be assigned to AWS resources, hence Kubernetes currently only supports assigning them to nodes.However, assigning a role to a node means that all processes running on that node will share the same role, which from the security point of view is not ideal asthe node may host different pods which require different permissions and we do not want have a super role in the system and share it between pods.  There are some solutions suggested by the community to assign a role to a pod but we didn’t want to add more elements to our cluster. We try to keep our cluster as simple as possible so we came up with a solution so that we can assign IAM roles directly to pods without having the overhead of using other services on Kubernetes. 

#### AWS IAM Role for Pods
As mentioned earlier, pods run on nodes and each node has a role on AWS, therefore the running pods share the same role as the hosting node. There is a feature in AWS IAM which enables a user or a resource with an IAM role to assume another role by creating a trust relationship between the assuming and assumed roles. Another feature that AWS IAM offers is to generate temporary credentials for each role. Using these two features we managed to assign the service IAM role we created to our K8s pod. For that we had to create a trust relationship between the node IAM role and the service IAM role. To assign the service IAM role to each pod we added a new environment variable to the service deployment template whose value is read from a K8s secret we created for the service. When a pod starts, the application reads the service IAM role from the environment variable, uses the AWS sdk to assume the service role, and then generates a temporary credentials for the service role. This way we can set the timespan of the temporary credentials to as short as 1 hr and either generate new credentials in a scheduled background process or even better, re-create new pods automatically. Bear in mind that it takes the application some time to generate new credentials and be ready to serve traffic, so you may want to implement kubernetes “pod readiness” to wait for the pod to be ready before directing traffic to it.
The benefit of generating temporary credentials is that you can store them in memory instead of environment variables which are not secure and can be viewed.

